{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Device analysis\n",
    "\n",
    "Now we will run an IV analysis on the device data we uploaded in the previous notebook using pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import gfhub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gfhub import nodes\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "user = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = gfhub.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Analysis function\n",
    "We can make an analysis function that runs on the data we just uploaded. This function does a linear fit between two columns in a dataframe and spits out a plot and a json with the fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fit(\n",
    "    path: Path,\n",
    "    /,\n",
    "    *,\n",
    "    xname: str,\n",
    "    yname: str,\n",
    "    slopename: str = \"resistance\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    ") -> tuple[Path, Path]:\n",
    "    \"\"\"Perform linear fit on IV data to extract resistance.\n",
    "\n",
    "    Args:\n",
    "        path: Path to parquet file with IV data\n",
    "        xname: Column name for x-axis (independent variable)\n",
    "        yname: Column name for y-axis (dependent variable)\n",
    "        slopename: Name for the extracted slope parameter\n",
    "        xlabel: Label for x-axis in plot\n",
    "        ylabel: Label for y-axis in plot\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (plot_path, results_path)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_parquet(path)\n",
    "    x = df[xname].values\n",
    "    y = df[yname].values\n",
    "\n",
    "    # Perform linear fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.scatter(x, y, alpha=0.6, label=\"Data\")\n",
    "    ax.plot(\n",
    "        x, slope * x + intercept, \"r-\", label=f\"Fit: y = {slope:.3e}x + {intercept:.3e}\"\n",
    "    )\n",
    "    xlabel = xlabel or xname\n",
    "    ylabel = ylabel or yname\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f\"{slopename} = {slope:.3e} (RÂ² = {r_value**2:.4f})\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Save plot\n",
    "    plot_path = path.with_name(path.stem + \"_linear_fit.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\", dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Save results as JSON\n",
    "    results = {\n",
    "        slopename: float(slope),\n",
    "        \"intercept\": float(intercept),\n",
    "        \"r_squared\": float(r_value**2),\n",
    "        \"p_value\": float(p_value),\n",
    "        \"std_err\": float(std_err),\n",
    "    }\n",
    "\n",
    "    results_path = path.with_name(path.stem + \"_linear_fit.json\")\n",
    "    results_path.write_text(json.dumps(results, indent=2))\n",
    "\n",
    "    return plot_path, results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "To make this function runnable on the server, we need to supply it with dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_def = gfhub.Function(\n",
    "    linear_fit,\n",
    "    dependencies={\n",
    "        \"pathlib\": \"from pathlib import Path\",\n",
    "        \"json\": \"import json\",\n",
    "        \"matplotlib\": \"import matplotlib.pyplot as plt\",\n",
    "        \"numpy\": \"import numpy as np\",\n",
    "        \"pandas[pyarrow]\": \"import pandas as pd\",\n",
    "        \"scipy\": \"from scipy import stats\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's test this function definition locally by running it in much the same way as the server will run it. We recommend doing this to prevent uploading broken functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should have been created in the previous notebook:\n",
    "path = Path(\"last_measurement.parquet\").resolve()\n",
    "\n",
    "result = func_def.eval(path, xname=\"current_mA\", yname=\"voltage_mV\")\n",
    "print(result)\n",
    "Image.open(result['output'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Once, confirmed it works as desired, we upload this function definition to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_function(\n",
    "    name=\"linear_fit\", \n",
    "    script=func_def,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Create pipeline\n",
    "\n",
    "Let's create a pipeline which invokes this function. This pipeline will essentially generate a .png (the plot) and a .json (the fit result) and link it to the source .parquet file that we're about to upload. By enabling the pipeline, anytime we upload additional parquet files with the right set of tags the pipeline will be triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gfhub.Pipeline()\n",
    "\n",
    "# we can manually trigger the pipeline\n",
    "p.trigger = nodes.on_manual_trigger()\n",
    "\n",
    "# or it will auto trigger when a file with these tags gets uploaded.\n",
    "# note that some of these tags don't have specified parameters values, \n",
    "# which means the trigger will activate for any of those values.\n",
    "p.auto_trigger = nodes.on_file_upload(\n",
    "    tags=[\n",
    "        \".parquet\",\n",
    "        user,\n",
    "        f\"project:resistance\",\n",
    "        \"wafer\",\n",
    "        \"die\",\n",
    "        \"cell\",\n",
    "        \"device\",\n",
    "        \"length\",\n",
    "        \"width\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# the triggers should trigger a file load and a tags load:\n",
    "p.load_file = nodes.load()\n",
    "p.load_tags = nodes.load_tags()\n",
    "\n",
    "# We connect nodes together with the `>>` operator:\n",
    "p += p.trigger >> p.load_file\n",
    "p += p.trigger >> p.load_tags\n",
    "p += p.auto_trigger >> p.load_file\n",
    "p += p.auto_trigger >> p.load_tags\n",
    "\n",
    "# after the file is loaded on disk, we'd like to run the analysis function:\n",
    "p.fit = nodes.function(\n",
    "    function=\"linear_fit\",\n",
    "    kwargs={\n",
    "        \"xname\": \"current_mA\",\n",
    "        \"yname\": \"voltage_mV\",\n",
    "        \"slopename\": \"resistance\",\n",
    "    },\n",
    ")\n",
    "p += p.load_file >> p.fit\n",
    "\n",
    "# the fit function has two outputs: the plot and the json:\n",
    "p.save_plot = nodes.save()\n",
    "p.save_json = nodes.save()\n",
    "# when a node has multiple ports they can be found using their index.\n",
    "p += p.fit[0] >> p.save_plot[0]\n",
    "p += p.fit[1] >> p.save_json[0]\n",
    "\n",
    "# the save nodes also have an input for tags:\n",
    "p += p.load_tags >> p.save_plot[1]\n",
    "p += p.load_tags >> p.save_json[1]\n",
    "\n",
    "pipeline_id = client.add_pipeline(name=\"device_linear_fit\", schema=p)[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "In human language, this pipeline will auto-gtrigger when a `.parquet` file with the right set of tags gets uploaded. However, we can also manually trigger it on any file.\n",
    "\n",
    "These triggers activate two load operations: `load` and `load_tags`. The first one saves the matching file on dist and the second one loads its associated tags (which we might want to use to save the function result with).\n",
    "\n",
    "The path to the file is then given to the function, which we wrote above. This function returns the path to the plot (index 0) and the path to a json containing the fit parameters (index 1).\n",
    "\n",
    "Both of these artifacts are then saved with separate save nodes. Save nodes have two input ports. One for the file to save (index 0) and one optional one for the tags to add to the file (index 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The pipeline can be viewed here. This nice visual representation should help validating that everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.pipeline_url(pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Trigger analysis for all devices\n",
    "Even though we configured this pipeline to run automatically on new files, we haven't run it yet for the files that we already uploaded in the previous notebook. Let's quickly trigger it for all previously uploaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_files = client.query_files(\n",
    "    tags=[\n",
    "        \".parquet\",\n",
    "        user,\n",
    "        f\"project:resistance\",\n",
    "        \"wafer\",\n",
    "        \"die\",\n",
    "        \"cell\",\n",
    "        \"device\",\n",
    "        \"length\",\n",
    "        \"width\",\n",
    "    ]\n",
    ")\n",
    "print(f\"Found {len(device_files)} device files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = []\n",
    "for device_file in tqdm(device_files):\n",
    "    triggered = client.trigger_pipeline(\"device_linear_fit\", device_file[\"id\"])\n",
    "    job_ids.extend(triggered[\"job_ids\"])\n",
    "\n",
    "print(f\"Triggered {len(job_ids)} analysis jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = client.wait_for_jobs(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query analysis plots\n",
    "analysis_plots = client.query_files(\n",
    "    name=\"*_linear_fit.png\",\n",
    "    tags=[f\"project:resistance\", user]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(analysis_plots)} analysis plots\")\n",
    "\n",
    "# Display the first plot\n",
    "if analysis_plots:\n",
    "    img = Image.open(client.download_file(analysis_plots[0]['id']))\n",
    "    display(img.resize((530, 400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Query analysis results (JSON files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query JSON results\n",
    "analysis_results = client.query_files(\n",
    "    name=\"*_linear_fit.json\", tags=[f\"project:resistance\", user]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(analysis_results)} analysis result files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "First result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_results:\n",
    "    result_buf = client.download_file(analysis_results[0][\"id\"])\n",
    "    result_data = json.load(result_buf)\n",
    "    print(json.dumps(result_data, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfhub",
   "language": "python",
   "name": "gfhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Die analysis\n",
    "\n",
    "Now we will run an analysis using the device data we uploaded in the previous notebook. \n",
    "\n",
    "As before, make sure you have the following environment variables set or added to a `.env` file:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import gfhub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from gfhub import nodes\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "user = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = gfhub.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Die analysis\n",
    "\n",
    "You can either trigger analysis automatically by defining it in the design manifest, using the UI or using the Python DoData library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = client.query_files(\n",
    "    tags=[\n",
    "        user,\n",
    "        \"die:-2,-3\",\n",
    "        \"device\",\n",
    "        \"project:cutback\",\n",
    "        \"cell\",\n",
    "        \"wafer\",\n",
    "        \".parquet\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    client.download_file(entry[\"id\"], f\"file_{i}.parquet\")\n",
    "    for i, entry in enumerate(entries)\n",
    "]\n",
    "dfs = [pd.read_parquet(path) for path in paths]\n",
    "tags = [\n",
    "    [\n",
    "        (k if not (p := v.get(\"parameter_value\")) else f\"{k}:{p}\")\n",
    "        for k, v in entry[\"tags\"].items()\n",
    "    ]\n",
    "    for entry in entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comps = [\n",
    "    int([t.replace(\"components:\", \"\") for t in ts if t.startswith(\"components:\")][0])\n",
    "    for ts in tags\n",
    "]\n",
    "powers = [df[\"power [dB]\"].max() for df in dfs]\n",
    "\n",
    "component_loss, insertion_loss = [-float(x) for x in np.polyfit(num_comps, powers, deg=1)]\n",
    "x = np.arange(0, max(num_comps) + 99, 100)\n",
    "plt.scatter(num_comps, powers, color=\"C1\")\n",
    "plt.plot(x, -component_loss * x - insertion_loss, color=\"C0\")\n",
    "plt.grid(visible=True)\n",
    "plt.xlim(x.min() - 30, x.max() + 30)\n",
    "plt.title(f\"loss = {component_loss:.2e} dB/component\")\n",
    "plt.xlabel(\"# components\")\n",
    "plt.ylabel(\"Power [dBm]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Analysis function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We can create our own DataLab function for this visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutback_die_analysis(\n",
    "    files: list[Path],\n",
    "    tags: list[list[str]],\n",
    "    /,\n",
    "    *,\n",
    "    output_name: str = \"cutback_die_analysis\",\n",
    ") -> tuple[Path, Path]:\n",
    "    \"\"\"Cutback die analysis.\"\"\"\n",
    "    dfs = [pd.read_parquet(file) for file in files]\n",
    "    num_comps = [\n",
    "        int(\n",
    "            [t.replace(\"components:\", \"\") for t in ts if t.startswith(\"components:\")][0]\n",
    "        )\n",
    "        for ts in tags\n",
    "    ]\n",
    "\n",
    "    powers = [df[\"power [dB]\"].max() for df in dfs]\n",
    "    component_loss, insertion_loss = [\n",
    "        -float(x) for x in np.polyfit(num_comps, powers, deg=1)\n",
    "    ]\n",
    "    x = np.arange(0, max(num_comps) + 99, 100)\n",
    "    plt.scatter(num_comps, powers, color=\"C1\")\n",
    "    plt.plot(x, -component_loss * x - insertion_loss, color=\"C0\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlim(x.min() - 30, x.max() + 30)\n",
    "    plt.title(f\"loss = {component_loss:.2e} dB/component\")\n",
    "    plt.xlabel(\"# components\")\n",
    "    plt.ylabel(\"Power [dBm]\")\n",
    "    path_plot = files[0].parent / f\"{output_name}.png\"\n",
    "    plt.savefig(path_plot, bbox_inches=\"tight\")\n",
    "    path_json = files[0].parent / f\"{output_name}.json\"\n",
    "    die_x, die_y = [\n",
    "        int(xy)\n",
    "        for xy in {k.split(\":\")[0]: k.split(\":\")[1] for k in tags[0] if \":\" in k}[\n",
    "            \"die\"\n",
    "        ].split(\",\")\n",
    "    ]\n",
    "    output = {\n",
    "        \"die_x\": die_x,\n",
    "        \"die_y\": die_y,\n",
    "        \"component_loss\": None if not np.isfinite(component_loss) else component_loss,\n",
    "        \"insertion_loss\": None if not np.isfinite(insertion_loss) else insertion_loss,\n",
    "    }\n",
    "    path_json.write_text(json.dumps(output))\n",
    "    return path_plot, path_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_def = gfhub.Function(\n",
    "    cutback_die_analysis,\n",
    "    dependencies={\n",
    "        \"numpy\": \"import numpy as np\",\n",
    "        \"pandas[pyarrow]\": \"import pandas as pd\",\n",
    "        \"json\": \"import json\",\n",
    "        \"matplotlib\": \"import matplotlib.pyplot as plt\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = func_def.eval(paths, tags)\n",
    "Image.open(result['output'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The function works, so let's upload it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_function('cutback_die_analysis', func_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Tag aggregation\n",
    "\n",
    "To accurately tag the output files, we create a simple function to merge common tags in a list of list of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_tags(\n",
    "    tags: list[list[str]],\n",
    "    /,\n",
    ") -> list[str]:\n",
    "    common = {}\n",
    "    for _tags in tags:\n",
    "        for t in _tags:\n",
    "            if \":\" in t:\n",
    "                key, value = t.split(\":\", 1)\n",
    "            else:\n",
    "                key, value = t, \"\"\n",
    "            if key not in common:\n",
    "                common[key] = set()\n",
    "            common[key].add(value)\n",
    "    common_tags = {k: list(v)[0] for k, v in common.items() if len(v) == 1}\n",
    "    return [k if not v else f\"{k}:{v}\" for k, v in common_tags.items() if not k.startswith('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Let's prepare a function definition we can upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_def = gfhub.Function(find_common_tags, dependencies={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's test this on the tags we loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_common_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_def.eval(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_function(\"find_common_tags\", func_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## A simple pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We can wrap this function in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gfhub.Pipeline()\n",
    "\n",
    "# a pipeline that takes a list of input paths (as opposed to a single input path)\n",
    "# cannot be configure to auto-trigger on upload. Therefore we only add a manual trigger:\n",
    "p.trigger = nodes.on_manual_trigger()\n",
    "\n",
    "# trigger kicks of a load from S3\n",
    "p.load_file = nodes.load()\n",
    "p += p.trigger >> p.load_file\n",
    "\n",
    "# it also kicks of a load of the tags\n",
    "p.load_tags = nodes.load_tags()\n",
    "p += p.trigger >> p.load_tags\n",
    "\n",
    "# the data file path (now on the local filesystem) as well as the\n",
    "# tags get passed to the analysis function\n",
    "p.cutback_die_analysis = nodes.function(function=\"cutback_die_analysis\")\n",
    "p += p.load_file >> p.cutback_die_analysis[0]\n",
    "p += p.load_tags >> p.cutback_die_analysis[1]\n",
    "\n",
    "# we also determine which tags all the data files have in common\n",
    "p.common_tags = nodes.function(function=\"find_common_tags\")\n",
    "p += p.load_tags >> p.common_tags\n",
    "\n",
    "# we save the plot with the common tags\n",
    "p.save_plot = nodes.save()\n",
    "p += p.cutback_die_analysis[0] >> p.save_plot[0]\n",
    "p += p.common_tags >> p.save_plot[1]\n",
    "\n",
    "# we save the json with the common tags\n",
    "p.save_json = nodes.save()\n",
    "p += p.cutback_die_analysis[1] >> p.save_json[0]\n",
    "p += p.common_tags >> p.save_json[1]\n",
    "\n",
    "# once the pipeline is defined, we can upload it:\n",
    "confirmation = client.add_pipeline(name=\"cutback_die_analysis\", schema=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "You can inspect the pipeline here after upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.pipeline_url(confirmation['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Trigger Pipeline\n",
    "\n",
    "To trigger the pipeline we can do a groupby and trigger it on groups of equal dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = client.query_files(\n",
    "    tags=[user, \"die\", \"device\", \"project:cutback\", \"cell\", \"wafer\", \".parquet\"]\n",
    ").groupby(\"die\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = []\n",
    "for tag, group in tqdm(entries.items()):\n",
    "    input_ids = [props['id'] for props in group]\n",
    "    triggered_jobs = client.trigger_pipeline(\"cutback_die_analysis\", input_ids)\n",
    "    job_ids.extend(triggered_jobs['job_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_jobs(job_ids);"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gfhub",
   "language": "python",
   "name": "gfhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
